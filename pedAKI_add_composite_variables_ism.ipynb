{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add composite variabels (SI, OSI, OI) to I/O dataframe for ISM\n",
    "* Query or load the dataframes that are required to to calculate the composite variables\n",
    "* Calculate the composite variabels\n",
    "* Add he composite variables to the original I/O dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pedAKI_utilities as paki\n",
    "# import ism_utilities_Ben as ism\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def queryDF4composites(encounter_ids, ism_instance=None, \n",
    "                       hr_df=None, nsbp_df=None, map_df=None, \n",
    "                       fio2_df=None, spo2_df=None, pao2_df=None):\n",
    "\n",
    "    try:\n",
    "        if hr_df is None:\n",
    "            hr_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_hr_df.pkl'))\n",
    "        hr_df = hr_df.loc[np.in1d(hr_df.encounter_id, encounter_ids),:]\n",
    "    except:\n",
    "        hr_df = ism_instance.getItemData(_itemid=602, _val_col=1, uom_str='bpm',\n",
    "                                         encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                            in_anno=True)\n",
    "#     print('hr dataframe queried ..')\n",
    "\n",
    "    try:\n",
    "        if nsbp_df is None:\n",
    "            nsbp_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_nsbp_df.pkl'))\n",
    "        nsbp_df = nsbp_df.loc[np.in1d(nsbp_df.encounter_id, encounter_ids), :]\n",
    "    except:\n",
    "        nsbp_df = ism_instance.getItemData(_itemid=764, _val_col=1, uom_str='mmHg',\n",
    "                                           encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                           in_anno=True)\n",
    "#     print('nsbp dataframe queried ..')\n",
    "\n",
    "    try:\n",
    "        if map_df is None:\n",
    "            map_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_map_df.pkl'))\n",
    "        map_df = map_df.loc[np.in1d(map_df.encounter_id, encounter_ids), :]\n",
    "    except:\n",
    "        map_df = ism_instance.getItemData(_itemid=746, _val_col=1, uom_str='cmH2O',\n",
    "                                          encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                          in_anno=True)\n",
    "#     print('map dataframe queried ..')\n",
    "\n",
    "    try:\n",
    "        if fio2_df is None:\n",
    "            fio2_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_fio2_df.pkl'))\n",
    "        fio2_df = fio2_df.loc[np.in1d(fio2_df.encounter_id, encounter_ids), :]\n",
    "    except:\n",
    "        fio2_df = ism_instance.getItemData(_itemid=501, _val_col=1, uom_str='fraction',\n",
    "                                           encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                           in_anno=True)\n",
    "#     print('fio2 dataframe queried ..')\n",
    "\n",
    "    try:\n",
    "        if spo2_df is None:\n",
    "            spo2_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_spo2_df.pkl'))\n",
    "        spo2_df = spo2_df.loc[np.in1d(spo2_df.encounter_id, encounter_ids),:]\n",
    "    except:\n",
    "        spo2_df1 = ism_instance.getItemData(_itemid=1065, _val_col=1, uom_str='%',\n",
    "                                            encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                            in_anno=True)\n",
    "        spo2_df2 = ism_instance.getItemData(_itemid=23540, _val_col=1, uom_str='%',\n",
    "                                            encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                            in_anno=True)\n",
    "        spo2_df = pd.concat([spo2_df1, spo2_df2], axis=0)        \n",
    "#     print('spo2 dataframe queried ..')\n",
    "\n",
    "    try:\n",
    "        if pao2_df is None:\n",
    "            pao2_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_pao2_df.pkl'))\n",
    "        pao2_df = pao2_df.loc[np.in1d(pao2_df.encounter_id, encounter_ids), :]\n",
    "    except:\n",
    "        pao2_df1 = ism_instance.getItemData(_itemid=1591, _val_col=1, uom_str='mmHg',\n",
    "                                            encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                            in_anno=True)\n",
    "        pao2_df2 = ism_instance.getItemData(_itemid=3103, _val_col=1, uom_str='mmHg',\n",
    "                                            encounter_list=encounter_ids, unique_census_time=True, \n",
    "                                            in_anno=True)\n",
    "        pao2_df = pd.concat([pao2_df1, pao2_df2], axis=0)        \n",
    "#     print('pao2 dataframe queried ..')\n",
    "    \n",
    "    return hr_df, nsbp_df, map_df, fio2_df, spo2_df, pao2_df\n",
    "\n",
    "def filtObservation(item_df, scr_df, timelag, timewin):\n",
    "    enc_reft = scr_df.groupby('encounter_id')['reftime'].unique().to_frame()\n",
    "    enc_reft = enc_reft.reset_index()\n",
    "    enc_reft['reftime'] = np.hstack(enc_reft.reftime)    \n",
    "    \n",
    "#     item_df = item_df.merge(scr_df.loc[:, ['encounter_id', 'reftime']], on='encounter_id', how='inner')\n",
    "    item_df = item_df.merge(enc_reft, on='encounter_id', how='inner')\n",
    "#     item_df.rename(columns={'tstamp': 'charttime'}, inplace=True)\n",
    "    if timelag > 0:\n",
    "        item_df['fromtime'] = item_df.reftime\n",
    "        item_df['totime'] = item_df.reftime + np.timedelta64(int(timelag * 60), 'm')\n",
    "        # time_mask = (_df.charttime > _df.reftime) & (_df.charttime < _df.totime)\n",
    "    else:\n",
    "        item_df['fromtime'] = item_df.reftime + np.timedelta64(int(timelag * 60), 'm')\n",
    "        item_df['totime'] = item_df.reftime + np.timedelta64(int((timelag + timewin) * 60), 'm')\n",
    "    time_mask = (item_df.charttime > item_df.fromtime) & (item_df.charttime < item_df.totime)\n",
    "    item_df = item_df.loc[time_mask, :]\n",
    "    \n",
    "    return item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convertMapUOM(map_df, val_col):\n",
    "    if map_df.uom.unique()[0] == 'mmHg':\n",
    "        eval(\"map_df.value\"+val_col+\"= map_df.value\"+val_col+\"*float(1.36)\")\n",
    "        map_df.uom = 'cmH2O'\n",
    "    return map_df\n",
    "\n",
    "def convertFio2UOM(fio2_df, val_col):\n",
    "    if fio2_df.uom.unique()[0] == '%':\n",
    "#         fio2_df.value = fio2_df.value*0.01\n",
    "        eval(\"fio2_df.value\"+val_col+\"= fio2_df.value\"+val_col+\"*0.01\")\n",
    "        fio2_df.uom = 'fraction'\n",
    "    return fio2_df\n",
    "\n",
    "def convertPao2UOM(pao2_df, val_col):\n",
    "    if pao2_df.uom.unique()[0] == 'kPa':\n",
    "#         pao2_df.value = pao2_df.value*7.5006\n",
    "        eval(\"pao2_df.value\"+val_col+\"= pao2_df.value\"+val_col+\"*7.5006\")\n",
    "        pao2_df.uom = 'mmHg'\n",
    "    return pao2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# si_df_filtered = hr_df_filtered.groupby('encounter_id').apply(getSI, nsbp_df_filtered)\n",
    "# si_df_filtered = si_df_filtered.loc[:, ['encounter_id', 'charttime', 'reftime', 'fromtime', 'totime', 'si']]\n",
    "# si_df_filtered.rename(columns={'si':'value'}, inplace=True)\n",
    "# si_df_filtered['valueUOM'] = 'bpm/mmHg'\n",
    "\n",
    "def getSI(hr_df_group, nsbp_df_filtered):\n",
    "    nsbp_df_group = nsbp_df_filtered.loc[nsbp_df_filtered.encounter_id\n",
    "                                         ==hr_df_group.encounter_id.unique()[0], :]\n",
    "    hr_df_group['si'] = np.nan\n",
    "    tdiff = np.timedelta64(60, 'm')\n",
    "    if not nsbp_df_group.empty:\n",
    "        for idx, row in hr_df_group.iterrows():\n",
    "            idxmin = abs(nsbp_df_group.charttime-row.charttime).idxmin()\n",
    "            valmin = abs(nsbp_df_group.charttime-row.charttime).min()\n",
    "            \n",
    "            if valmin<=np.timedelta64(60,'m'):\n",
    "                try:\n",
    "                    hr_df_group.si[idx] = row.value1/float(nsbp_df_group.value1[idxmin])\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return hr_df_group\n",
    "\n",
    "def getSIDF(hr_df, nsbp_df):\n",
    "    si_df = hr_df.groupby('encounter_id').apply(getSI, nsbp_df)\n",
    "    si_df = si_df.loc[:, ['encounter_id', 'charttime', 'reftime', 'fromtime', 'totime', 'si']]\n",
    "    si_df.rename(columns={'si':'value'}, inplace=True)\n",
    "    si_df['uom'] = 'bpm/mmHg'\n",
    "    si_df = si_df.replace([np.inf, -np.inf], np.nan)\n",
    "    return si_df\n",
    "    \n",
    "def getOSI(spo2_df_group, map_df_filtered, fio2_df_filtered):\n",
    "    map_df_group = map_df_filtered.loc[map_df_filtered.encounter_id\n",
    "                                       ==spo2_df_group.encounter_id.unique()[0], :]\n",
    "    fio2_df_group = fio2_df_filtered.loc[fio2_df_filtered.encounter_id\n",
    "                                         ==spo2_df_group.encounter_id.unique()[0], :]\n",
    "    spo2_df_group['osi'] = np.nan\n",
    "    tdiff = np.timedelta64(60, 'm')\n",
    "    if not (map_df_group.empty or fio2_df_group.empty):\n",
    "        for idx, row in spo2_df_group.iterrows():\n",
    "            idxmin_map = abs(map_df_group.charttime-row.charttime).idxmin()\n",
    "            valmin_map = abs(map_df_group.charttime-row.charttime).min()\n",
    "            idxmin_fio2 = abs(fio2_df_group.charttime-row.charttime).idxmin()\n",
    "            valmin_fio2 = abs(fio2_df_group.charttime-row.charttime).min()\n",
    "            \n",
    "            if (valmin_map<=np.timedelta64(60,'m')) and (valmin_fio2<=np.timedelta64(60,'m')):\n",
    "                try:\n",
    "                    spo2_df_group.osi[idx] = (map_df_group.value1[idxmin_map]\n",
    "                                              *fio2_df_group.value1[idxmin_fio2]*100)/float(row.value1)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return spo2_df_group\n",
    "\n",
    "def getOSIDF(spo2_df, map_df, fio2_df):\n",
    "    osi_df = spo2_df.groupby('encounter_id').apply(getOSI, map_df, fio2_df)\n",
    "    osi_df = osi_df.loc[:, ['encounter_id', 'charttime', 'reftime', 'fromtime', 'totime', 'osi']]\n",
    "    osi_df.rename(columns={'osi':'value'}, inplace=True)\n",
    "    osi_df['uom'] = 'cmH2O'\n",
    "    osi_df = osi_df.replace([np.inf, -np.inf], np.nan)\n",
    "    return osi_df\n",
    "\n",
    "def getOI(pao2_df_group, map_df_filtered, fio2_df_filtered):\n",
    "    map_df_group = map_df_filtered.loc[map_df_filtered.encounter_id\n",
    "                                       ==pao2_df_group.encounter_id.unique()[0], :]\n",
    "    fio2_df_group = fio2_df_filtered.loc[fio2_df_filtered.encounter_id\n",
    "                                         ==pao2_df_group.encounter_id.unique()[0], :]\n",
    "    pao2_df_group['oi'] = np.nan\n",
    "    tdiff = np.timedelta64(60, 'm')\n",
    "    if not (map_df_group.empty or fio2_df_group.empty):\n",
    "        for idx, row in pao2_df_group.iterrows():\n",
    "            idxmin_map = abs(map_df_group.charttime-row.charttime).idxmin()\n",
    "            valmin_map = abs(map_df_group.charttime-row.charttime).min()\n",
    "            idxmin_fio2 = abs(fio2_df_group.charttime-row.charttime).idxmin()\n",
    "            valmin_fio2 = abs(fio2_df_group.charttime-row.charttime).min()\n",
    "            \n",
    "            if (valmin_map<=np.timedelta64(60,'m')) and (valmin_fio2<=np.timedelta64(60,'m')):\n",
    "                try:\n",
    "                    pao2_df_group.oi[idx] = (map_df_group.value1[idxmin_map]\n",
    "                                             *fio2_df_group.value1[idxmin_fio2]*100)/float(row.value1*1.36)\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return pao2_df_group\n",
    "\n",
    "def getOIDF(pao2_df, map_df, fio2_df):\n",
    "    oi_df = pao2_df.groupby('encounter_id').apply(getOI, map_df, fio2_df)\n",
    "    oi_df = oi_df.loc[:, ['encounter_id', 'charttime', 'reftime', 'fromtime', 'totime', 'oi']]\n",
    "    oi_df.rename(columns={'oi':'value'}, inplace=True)\n",
    "    oi_df['uom'] = '%'\n",
    "    oi_df = oi_df.replace([np.inf, -np.inf], np.nan)\n",
    "    return oi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLastValDF(group):\n",
    "    \n",
    "    tmp_group = group.loc[~pd.isnull(group.value),:]\n",
    "    if not tmp_group.empty:\n",
    "        return tmp_group.value[tmp_group.charttime.idxmax()]\n",
    "    else:\n",
    "        return np.NaN\n",
    "    \n",
    "def add2IO(io_df, feature_dfs, stats):\n",
    "    feature_stats = [(feature, stat) for feature in feature_dfs for stat in stats]\n",
    "    for feature, stat in feature_stats:\n",
    "        feature_df = feature_dfs[feature]\n",
    "        ft_name = feature+'_'+stat\n",
    "#         print(ft_name)\n",
    "        if stat !='last':\n",
    "            ft_stat_df = eval(\"feature_df.groupby('encounter_id')['value'].\"+stat+\"().reset_index()\")\n",
    "        else:\n",
    "            ft_stat_sr = feature_df.groupby('encounter_id').apply(getLastValDF)\n",
    "            ft_stat_df = pd.DataFrame(ft_stat_sr)\n",
    "            ft_stat_df.rename(columns={0: 'value'}, inplace=True)\n",
    "            ft_stat_df = ft_stat_df.reset_index()\n",
    "            \n",
    "        io_df = io_df.merge(ft_stat_df, on='encounter_id', how='left')\n",
    "        io_df.rename(columns={'value': ft_name}, inplace=True)\n",
    "    return io_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fileDir = os.path.dirname(\"__file__\")\n",
    "timelag = np.arange(-6,-25,-1)\n",
    "timewin = [6, 12]\n",
    "stabletime = list([12])\n",
    "combination = [(tlag, twin, stime) for tlag in timelag for twin in timewin \n",
    "               for stime in stabletime if abs(tlag)>=twin]\n",
    "# combination = combination[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-19, 12, 12), (-20, 6, 12), (-20, 12, 12), (-21, 6, 12), (-21, 12, 12), (-22, 6, 12), (-22, 12, 12), (-23, 6, 12), (-23, 12, 12), (-24, 6, 12), (-24, 12, 12)]\n"
     ]
    }
   ],
   "source": [
    "combination = combination[-11:]\n",
    "print(combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timelag: -19, timewindow: 12\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -19, timewindow: 12 finished!! \n",
      "\n",
      "timelag: -20, timewindow: 6\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -20, timewindow: 6 finished!! \n",
      "\n",
      "timelag: -20, timewindow: 12\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -20, timewindow: 12 finished!! \n",
      "\n",
      "timelag: -21, timewindow: 6\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -21, timewindow: 6 finished!! \n",
      "\n",
      "timelag: -21, timewindow: 12\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -21, timewindow: 12 finished!! \n",
      "\n",
      "timelag: -22, timewindow: 6\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -22, timewindow: 6 finished!! \n",
      "\n",
      "timelag: -22, timewindow: 12\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -22, timewindow: 12 finished!! \n",
      "\n",
      "timelag: -23, timewindow: 6\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -23, timewindow: 6 finished!! \n",
      "\n",
      "timelag: -23, timewindow: 12\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -23, timewindow: 12 finished!! \n",
      "\n",
      "timelag: -24, timewindow: 6\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -24, timewindow: 6 finished!! \n",
      "\n",
      "timelag: -24, timewindow: 12\n",
      "Queried dataframes ...\n",
      "Required dataframes filtered by observation window ...\n",
      "Got Shock Index ...\n",
      "Got Oxygenation Saturation Index ...\n",
      "Got Oxygenation Index ...\n",
      "Added to the I/O dataframe\n",
      "timelag: -24, timewindow: 12 finished!! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "reload(paki)\n",
    "\n",
    "hr_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_hr_df.pkl'))\n",
    "nsbp_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_nsbp_df.pkl'))\n",
    "map_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_map_df.pkl'))\n",
    "fio2_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_fio2_df.pkl'))\n",
    "spo2_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_spo2_df.pkl'))\n",
    "pao2_df = pd.read_pickle(os.path.join(fileDir, 'item_df_ism', 'ism_pao2_df.pkl'))\n",
    "\n",
    "if not os.path.exists(os.path.join(fileDir, 'io_ism2')):\n",
    "        os.makedirs(os.path.join(fileDir, 'io_ism2'))\n",
    "        \n",
    "for tlag, twin, stime in combination:\n",
    "    \n",
    "    print('timelag: {}, timewindow: {}'.format(tlag, twin))\n",
    "    \n",
    "    fname_scr_df = os.path.join(fileDir, 'scr_ism', \n",
    "                                'ism_onset_scr_tlag{:03d}_stime{:03d}_tot.pkl'.format(abs(tlag), stime))\n",
    "    fname_io_df_aki = os.path.join(fileDir, 'io_ism', \n",
    "                                   'ism_onset_io_tlag{:03d}_twin{:03d}_aki.pkl'.format(abs(tlag), twin))\n",
    "    fname_io_df_con = os.path.join(fileDir, 'io_ism', \n",
    "                                   'ism_onset_io_tlag{:03d}_twin{:03d}_con.pkl'.format(abs(tlag), twin))\n",
    "    \n",
    "    # Tag reference time to the creatinine dataframe\n",
    "    scr_df = pd.read_pickle(fname_scr_df)\n",
    "    if 'reftime' not in scr_df.columns:\n",
    "        scr_df = paki.cleanSCrDf2(scr_df, ex_age=True, ex_los=True, \n",
    "                                  ex_aki_adm=True, aki_adm_hr=max(12,abs(tlag)), enc_per_pat=True)\n",
    "        scr_df.to_pickle(fname_scr_df)\n",
    "    \n",
    "    # Load I/O matrix\n",
    "    io_df_aki = pd.read_pickle(fname_io_df_aki)\n",
    "    io_df_con = pd.read_pickle(fname_io_df_con)\n",
    "    \n",
    "    # Query dataframes to calculate the composite predictors\n",
    "    encounter_ids_aki = io_df_aki.encounter_id.unique()\n",
    "    encounter_ids_con = io_df_con.encounter_id.unique()\n",
    "    \n",
    "    (hr_df_aki, nsbp_df_aki, map_df_aki, \n",
    "     fio2_df_aki, spo2_df_aki, pao2_df_aki) = queryDF4composites(encounter_ids_aki,\n",
    "                                                                 hr_df=hr_df, nsbp_df=nsbp_df, \n",
    "                                                                 map_df=map_df, fio2_df=fio2_df, \n",
    "                                                                 spo2_df=spo2_df, pao2_df=pao2_df)\n",
    "    (hr_df_con, nsbp_df_con, map_df_con, \n",
    "     fio2_df_con, spo2_df_con, pao2_df_con) = queryDF4composites(encounter_ids_con,\n",
    "                                                                 hr_df=hr_df, nsbp_df=nsbp_df, \n",
    "                                                                 map_df=map_df, fio2_df=fio2_df, \n",
    "                                                                 spo2_df=spo2_df, pao2_df=pao2_df)\n",
    "    print('Queried dataframes ...')\n",
    "    \n",
    "    # Filter dataframes with the observation window\n",
    "    hr_df_filtered_aki = filtObservation(hr_df_aki, scr_df, tlag, twin)\n",
    "    nsbp_df_filtered_aki = filtObservation(nsbp_df_aki, scr_df, tlag, twin)\n",
    "    map_df_filtered_aki = filtObservation(map_df_aki, scr_df, tlag, twin)\n",
    "    fio2_df_filtered_aki = filtObservation(fio2_df_aki, scr_df, tlag, twin)\n",
    "    spo2_df_filtered_aki = filtObservation(spo2_df_aki, scr_df, tlag, twin)\n",
    "    pao2_df_filtered_aki = filtObservation(pao2_df_aki, scr_df, tlag, twin)\n",
    "    \n",
    "    hr_df_filtered_con = filtObservation(hr_df_con, scr_df, tlag, twin)\n",
    "    nsbp_df_filtered_con = filtObservation(nsbp_df_con, scr_df, tlag, twin)\n",
    "    map_df_filtered_con = filtObservation(map_df_con, scr_df, tlag, twin)\n",
    "    fio2_df_filtered_con = filtObservation(fio2_df_con, scr_df, tlag, twin)\n",
    "    spo2_df_filtered_con = filtObservation(spo2_df_con, scr_df, tlag, twin)\n",
    "    pao2_df_filtered_con = filtObservation(pao2_df_con, scr_df, tlag, twin)\n",
    "    print('Required dataframes filtered by observation window ...')\n",
    "    \n",
    "        \n",
    "    # Convert UOM to be consistent with ISM\n",
    "#     map_df_filtered_aki = convertMapUOM(map_df_filtered_aki)\n",
    "#     map_df_filtered_con = convertMapUOM(map_df_filtered_con)\n",
    "    \n",
    "#     fio2_df_filtered_aki = convertFio2UOM(fio2_df_filtered_aki)\n",
    "#     fio2_df_filtered_con = convertFio2UOM(fio2_df_filtered_con)\n",
    "    \n",
    "#     pao2_df_filtered_aki = convertPao2UOM(pao2_df_filtered_aki)\n",
    "#     pao2_df_filtered_con = convertPao2UOM(pao2_df_filtered_con)\n",
    "#     print('UOM converted ...')\n",
    "    \n",
    "    # Get Shock index (SI) dataframe\n",
    "    si_df_filtered_aki = getSIDF(hr_df_filtered_aki, nsbp_df_filtered_aki)\n",
    "    si_df_filtered_con = getSIDF(hr_df_filtered_con, nsbp_df_filtered_con)\n",
    "    print('Got Shock Index ...')\n",
    "    \n",
    "    \n",
    "    # Get Oxygenation Saturation Index (OSI) dataframe\n",
    "    osi_df_filtered_aki = getOSIDF(spo2_df_filtered_aki, map_df_filtered_aki, \n",
    "                                   fio2_df_filtered_aki)\n",
    "    osi_df_filtered_con = getOSIDF(spo2_df_filtered_con, map_df_filtered_con, \n",
    "                                   fio2_df_filtered_con)\n",
    "    print('Got Oxygenation Saturation Index ...')\n",
    "    \n",
    "    # Get Oxygenation Index (OI) dataframe\n",
    "    oi_df_filtered_aki = getOIDF(pao2_df_filtered_aki, map_df_filtered_aki,\n",
    "                                 fio2_df_filtered_aki)\n",
    "    oi_df_filtered_con = getOIDF(pao2_df_filtered_con, map_df_filtered_con,\n",
    "                                 fio2_df_filtered_con)\n",
    "    print('Got Oxygenation Index ...')\n",
    "    \n",
    "    # Add feature statistics to the I/O dataframe\n",
    "    feature_dfs_aki = {'si': si_df_filtered_aki, 'osi': osi_df_filtered_aki, 'oi': oi_df_filtered_aki}\n",
    "    feature_dfs_con = {'si': si_df_filtered_con, 'osi': osi_df_filtered_con, 'oi': oi_df_filtered_con}\n",
    "    stats = ['min', 'max', 'mean', 'last', 'median']\n",
    "    \n",
    "    io_df_full_aki = add2IO(io_df_aki, feature_dfs_aki, stats)\n",
    "    io_df_full_con = add2IO(io_df_con, feature_dfs_con, stats)\n",
    "    print('Added to the I/O dataframe')\n",
    "    \n",
    "    \n",
    "    fname_io_full_aki2 = os.path.join(fileDir, 'io_ism2', \n",
    "                                      'ism_onset_io_tlag{:03d}_twin{:03d}_aki.pkl'.format(abs(tlag), twin))\n",
    "    fname_io_full_con2 = os.path.join(fileDir, 'io_ism2', \n",
    "                                      'ism_onset_io_tlag{:03d}_twin{:03d}_con.pkl'.format(abs(tlag), twin))\n",
    "    io_df_full_aki.to_pickle(fname_io_full_aki2)\n",
    "    io_df_full_con.to_pickle(fname_io_full_con2)\n",
    "    \n",
    "    \n",
    "    print('timelag: {}, timewindow: {} finished!! \\n'.format(tlag, twin))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
